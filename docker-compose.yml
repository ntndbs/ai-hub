services:
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    platform: linux/amd64
    ports:
      - "4000:4000"
    environment:
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      LITELLM_LOG: info
    volumes:
      # Ã¼berschreibt die DEFAULT config, die LiteLLM offenbar wirklich nutzt
      - ./litellm_config.yml:/app/proxy_server_config.yaml
    command: ["--config", "/app/proxy_server_config.yaml", "--port", "4000", "--host", "0.0.0.0"]

  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "8501:8080"
    environment:
      OPENAI_API_BASE_URL: http://litellm:4000/v1
      OPENAI_API_KEY: dummy
    depends_on:
      - litellm
    volumes:
      - openwebui:/app/backend/data

volumes:
  openwebui: